---
title: "Web Scraping using rvest package"
author: "Davut Ayan"
date: '2021-08-21'
slug: first post
categories: ["R"]
tags: ["R", "Web Scraping", "rvest"]
subtitle: ''
summary: 'This project showcases how `rvest` package in `R` can be utilized to scrape data from a website.'
lastmod: "`r Sys.Date()`"
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
library(dplyr)  ## for data manipulation
```



**Loading required packages**
    
```{r}
library(rvest)  ## web scraping
```

Here I read data from AEA website urls using *read_html()* function.

```{r}
url1 <- "https://www.aeaweb.org/about-aea/committees/csmgep/neacode-a-l"
url2 <- "https://www.aeaweb.org/about-aea/committees/csmgep/neacode-m-z"
simple <- read_html(url1)
simple2 <- read_html(url2)
simple
```

As seen above, downloaded data is list of *head* and *body*. Understanding the nature of this data requires at least basic literacy of html documents. Data is structured like a tree and branches. Each point in data, denoted node, has different category of information.

Before we delve into downloaded data, it is wise to see the website we want to scrape in developer mode (inspect element) so that we can understand how the website is structured and what type of information exists.

Below you can see what values are contained in node "p", also check the website and see what the tag \<p> contains.

```{r}
simple %>%
    html_nodes("p") %>%
    html_text()
```

I can extract column names (variable names) from the node "th". There are five columns and I can extract 5 column names. 

```{r}
names <- 
simple %>%
    html_nodes("th") %>%
    html_text()

names <- make.names(names)  # quickly convert column names
names
```

And below I extract the main data from the node "td". Data is extracted into a vector named "vec".

```{r}
vec <-
simple %>%
    html_nodes("td") %>%
    html_text()

head(vec)
```

Then I can convert the vector in a matrix format. There are five columns and number of rows are computed automatically.

```{r}
mat <- 
    matrix(vec, nrow = round(length(vec)/5), ncol = 5, byrow = T)
mat[1:5, 1:4]
```

And finally, I can convert the matrix to a data frame attaching variable names.

```{r}
df1 <- as.data.frame(mat)
names(df1) <- names

head(df1, n=5)
```

Same procedure is followed for the second url which contains the rest of the data.

```{r}
names2 <-
simple2 %>%
    html_nodes("th") %>%
    html_text()
names2 <- make.names(names2)  # quickly convert column names
names2
```

```{r}
vec2 <-
simple2 %>%
    html_nodes("td") %>%
    html_text()
head(vec2)
```

```{r}
mat2 <- 
    matrix(vec2, nrow = round(length(vec2)/5), ncol = 5, byrow = T)
mat2[1:5, 1:4]
```


```{r}
df2 <- as.data.frame(mat)
names(df2) <- names

head(df2, n=5)
```
And we can append (merge) those files and make one complete data frame with all African American economists in the US.

Among many ways to append data frames sharing the same column names, I show below merging (appending in STATA) using dplyr package, particularly calling bind_rows() function because of its advantages over other methods.

[rbind.data.frame(), do.call(rbind, ) ... ]

```{r}
df <- bind_rows(df1, df2)

head(df, n=5)
```




